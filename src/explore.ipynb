{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "import joblib # For saving the model and preprocessors\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- 1.1: Loading the Titanic dataset ---\n",
                        "Titanic dataset loaded successfully.\n",
                        "DataFrame head:\n",
                        "   PassengerId  Survived  Pclass  \\\n",
                        "0            1         0       3   \n",
                        "1            2         1       1   \n",
                        "2            3         1       3   \n",
                        "3            4         1       1   \n",
                        "4            5         0       3   \n",
                        "\n",
                        "                                                Name     Sex   Age  SibSp  \\\n",
                        "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
                        "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
                        "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
                        "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
                        "4                           Allen, Mr. William Henry    male  35.0      0   \n",
                        "\n",
                        "   Parch            Ticket     Fare Cabin Embarked  \n",
                        "0      0         A/5 21171   7.2500   NaN        S  \n",
                        "1      0          PC 17599  71.2833   C85        C  \n",
                        "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
                        "3      0            113803  53.1000  C123        S  \n",
                        "4      0            373450   8.0500   NaN        S  \n",
                        "\n",
                        "DataFrame info:\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 891 entries, 0 to 890\n",
                        "Data columns (total 12 columns):\n",
                        " #   Column       Non-Null Count  Dtype  \n",
                        "---  ------       --------------  -----  \n",
                        " 0   PassengerId  891 non-null    int64  \n",
                        " 1   Survived     891 non-null    int64  \n",
                        " 2   Pclass       891 non-null    int64  \n",
                        " 3   Name         891 non-null    object \n",
                        " 4   Sex          891 non-null    object \n",
                        " 5   Age          714 non-null    float64\n",
                        " 6   SibSp        891 non-null    int64  \n",
                        " 7   Parch        891 non-null    int64  \n",
                        " 8   Ticket       891 non-null    object \n",
                        " 9   Fare         891 non-null    float64\n",
                        " 10  Cabin        204 non-null    object \n",
                        " 11  Embarked     889 non-null    object \n",
                        "dtypes: float64(2), int64(5), object(5)\n",
                        "memory usage: 83.7+ KB\n",
                        "\n",
                        "Survival (target) value counts:\n",
                        "Survived\n",
                        "0    549\n",
                        "1    342\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "# --- 1.1: Load the Titanic dataset ---\n",
                "print(\"--- 1.1: Loading the Titanic dataset ---\")\n",
                "# Using a direct URL for convenience, similar to previous projects\n",
                "titanic_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
                "\n",
                "try:\n",
                "    df = pd.read_csv(titanic_url)\n",
                "    print(\"Titanic dataset loaded successfully.\")\n",
                "    print(\"DataFrame head:\")\n",
                "    print(df.head())\n",
                "    print(\"\\nDataFrame info:\")\n",
                "    df.info()\n",
                "    print(\"\\nSurvival (target) value counts:\")\n",
                "    print(df['Survived'].value_counts())\n",
                "except Exception as e:\n",
                "    print(f\"Error loading Titanic dataset: {e}\")\n",
                "    print(\"Please ensure the URL is correct or check your internet connection.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- 1.2: Basic EDA and Preprocessing ---\n",
                        "Dropped 'PassengerId', 'Name', 'Ticket', 'Cabin' columns.\n",
                        "Handled missing values in 'Age' (median) and 'Embarked' (mode).\n",
                        "\n",
                        "Missing values after handling:\n",
                        "Survived    0\n",
                        "Pclass      0\n",
                        "Sex         0\n",
                        "Age         0\n",
                        "SibSp       0\n",
                        "Parch       0\n",
                        "Fare        0\n",
                        "Embarked    0\n",
                        "dtype: int64\n",
                        "\n",
                        "Shape of X: (891, 7)\n",
                        "Shape of y: (891,)\n",
                        "\n",
                        "Preprocessing pipelines defined.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_42496/3876678380.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
                        "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
                        "\n",
                        "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
                        "\n",
                        "\n",
                        "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
                        "/tmp/ipykernel_42496/3876678380.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
                        "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
                        "\n",
                        "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
                        "\n",
                        "\n",
                        "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
                    ]
                }
            ],
            "source": [
                "# --- 1.2: Basic EDA and Preprocessing ---\n",
                "print(\"\\n--- 1.2: Basic EDA and Preprocessing ---\")\n",
                "\n",
                "# Drop irrelevant columns for this basic model\n",
                "# 'PassengerId', 'Name', 'Ticket', 'Cabin' are often dropped or require complex processing\n",
                "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
                "print(\"Dropped 'PassengerId', 'Name', 'Ticket', 'Cabin' columns.\")\n",
                "\n",
                "# Handle missing values\n",
                "# 'Age': Fill with median\n",
                "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
                "# 'Embarked': Fill with mode (most frequent value)\n",
                "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
                "print(\"Handled missing values in 'Age' (median) and 'Embarked' (mode).\")\n",
                "\n",
                "print(\"\\nMissing values after handling:\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# Define features (X) and target (y)\n",
                "X = df.drop('Survived', axis=1)\n",
                "y = df['Survived']\n",
                "print(f\"\\nShape of X: {X.shape}\")\n",
                "print(f\"Shape of y: {y.shape}\")\n",
                "\n",
                "# Identify numerical and categorical features for preprocessing pipelines\n",
                "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
                "categorical_features = ['Pclass', 'Sex', 'Embarked'] # Pclass is numerical but treated as categorical here\n",
                "\n",
                "# Create preprocessing pipelines for numerical and categorical features\n",
                "# Numerical pipeline: Impute (if not already done) and Scale\n",
                "numerical_transformer = Pipeline(steps=[\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# Categorical pipeline: One-hot encode\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' for unseen categories in test set\n",
                "])\n",
                "\n",
                "# Create a preprocessor using ColumnTransformer\n",
                "# This applies different transformations to different columns\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_features),\n",
                "        ('cat', categorical_transformer, categorical_features)\n",
                "    ])\n",
                "\n",
                "print(\"\\nPreprocessing pipelines defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- 1.3: Training a Logistic Regression model ---\n",
                        "Shape of X_train: (712, 7)\n",
                        "Shape of X_test: (179, 7)\n",
                        "Training the model...\n",
                        "Model training complete.\n"
                    ]
                }
            ],
            "source": [
                "# --- 1.3: Train a simple classification model (Logistic Regression) ---\n",
                "print(\"\\n--- 1.3: Training a Logistic Regression model ---\")\n",
                "\n",
                "# Combine preprocessing and model into a single pipeline\n",
                "model_pipeline = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', LogisticRegression(random_state=42, max_iter=1000)) # Increased max_iter for convergence\n",
                "])\n",
                "\n",
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "print(f\"Shape of X_train: {X_train.shape}\")\n",
                "print(f\"Shape of X_test: {X_test.shape}\")\n",
                "\n",
                "# Train the model\n",
                "print(\"Training the model...\")\n",
                "model_pipeline.fit(X_train, y_train)\n",
                "print(\"Model training complete.\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- 1.4: Evaluating the model ---\n",
                        "Accuracy: 0.8045\n",
                        "Classification Report:\n",
                        "               precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.81      0.89      0.85       110\n",
                        "           1       0.79      0.67      0.72        69\n",
                        "\n",
                        "    accuracy                           0.80       179\n",
                        "   macro avg       0.80      0.78      0.79       179\n",
                        "weighted avg       0.80      0.80      0.80       179\n",
                        "\n",
                        "Confusion Matrix:\n",
                        " [[98 12]\n",
                        " [23 46]]\n"
                    ]
                }
            ],
            "source": [
                "# --- 1.4: Evaluate the model ---\n",
                "print(\"\\n--- 1.4: Evaluating the model ---\")\n",
                "y_pred = model_pipeline.predict(X_test)\n",
                "\n",
                "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
                "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
                "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- 1.5: Saving the model and preprocessors ---\n",
                        "Full model pipeline saved to models/titanic_survival_predictor_pipeline.joblib\n",
                        "\n",
                        "Model training and saving complete. You can now proceed to Step 2 to build your Streamlit app.\n"
                    ]
                }
            ],
            "source": [
                "# --- 1.5: Save the model and preprocessors ---\n",
                "print(\"\\n--- 1.5: Saving the model and preprocessors ---\")\n",
                "\n",
                "# Ensure the 'models' directory exists\n",
                "models_dir = 'models/'\n",
                "os.makedirs(models_dir, exist_ok=True)\n",
                "\n",
                "# Save the entire pipeline (which includes the preprocessor and classifier)\n",
                "model_filename = os.path.join(models_dir, 'titanic_survival_predictor_pipeline.joblib')\n",
                "joblib.dump(model_pipeline, model_filename)\n",
                "print(f\"Full model pipeline saved to {model_filename}\")\n",
                "\n",
                "# Note: When saving the entire pipeline, you don't need to save scaler/onehotencoder separately,\n",
                "# as they are part of the pipeline.\n",
                "\n",
                "print(\"\\nModel training and saving complete. You can now proceed to Step 2 to build your Streamlit app.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
